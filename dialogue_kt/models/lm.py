import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

from dialogue_kt.utils import get_checkpoint_path

bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

def get_base_model(base_model_name: str, tokenizer: AutoTokenizer, quantize: bool):
    # print("QUANTIZE:", quantize)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        pad_token_id=tokenizer.pad_token_id,
        quantization_config=bnb_config if quantize else None,
        # f32 seems helpful for train/test time consistency when quantizing, bf16 performs best for non-quantized
        torch_dtype=torch.float32 if quantize else torch.bfloat16,
        device_map="auto" if quantize else {"": 0}
    )
    base_model.config.use_cache = False
    base_model.config.pretraining_tp = 1
    return base_model

def get_model(base_model_name: str, test: bool,
              model_name: str = None, pt_model_name: str = None,
              r: int = None, lora_alpha: int = None,
              quantize: bool = True, use_gradient_checkpointing: bool = True):
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side="right")
    tokenizer.pad_token = tokenizer.bos_token # Have to pick some token, and eos triggers a warning
    model = get_base_model(base_model_name, tokenizer, quantize)
    if test and model_name:
        # Note we are loading adapter on quantized model and not merging
        # Recommended here - https://huggingface.co/docs/trl/main/en/dpo_trainer#downsides-to-merging-qlora-before-dpo-approach-2
        # Also prevents empty responses generated by Llama models
        print("Initializing inference-time model from fine-tuned LoRA adapters")
        model = PeftModel.from_pretrained(model, get_checkpoint_path(model_name))
    elif not test:
        if quantize:
            model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)
        if pt_model_name:
            print("Initializing trainable model from pre-trained LoRA adapters")
            model = PeftModel.from_pretrained(model, get_checkpoint_path(pt_model_name), is_trainable=True, adapter_name="default")
        else:
            print("Initializing trainable model with new LoRA adapters")
            peft_config = LoraConfig(
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                r=r,
                lora_alpha=lora_alpha,
                lora_dropout=0.05,
                task_type="CAUSAL_LM",
                inference_mode=False,
            )
            model = get_peft_model(model, peft_config)
    else:
        print("Initializing inference-time model from pre-trained weights")
    return model, tokenizer
