No conda package cache directories found outside your home directory. To
prevent conda from filling up your home directory, you can create a new
directory at `/work/pi_<your_pi_name>/$USER/.conda/pkgs` and reload the module. 
No conda environment directories found outside your home directory. To prevent
conda from filling up your home directory, you can create a new directory at
`/work/pi_<your_pi_name>/$USER/.conda/envs` and reload the module. 
Loading conda version miniforge3-24.7.1
Loading cuda version 12.6
Loading cudnn version 8.9.7.29-12
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:19,  9.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:28<00:09,  9.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  5.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  7.26s/it]
Training:   0%|          | 0/459 [00:00<?, ?it/s]/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/backends/default/ops.py:68: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  output = output.addmm(subA, subB)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/nn/modules/linear.py:134: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return F.linear(input, self.weight, self.bias)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:290: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  grad_A = torch.matmul(grad_output.to(ctx.dtype_A), CB).view(ctx.grad_shape)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/autograd/graph.py:841: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/autograd/graph.py:841: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:897.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Training:   0%|          | 1/459 [00:02<17:47,  2.33s/it]Training:   0%|          | 2/459 [00:03<13:45,  1.81s/it]Training:   1%|          | 3/459 [00:05<13:15,  1.74s/it]Training:   1%|          | 4/459 [00:07<13:13,  1.74s/it]Training:   1%|          | 5/459 [00:09<14:33,  1.92s/it]Training:   1%|▏         | 6/459 [00:11<14:08,  1.87s/it]Training:   2%|▏         | 7/459 [00:12<13:03,  1.73s/it]Training:   2%|▏         | 8/459 [00:17<20:25,  2.72s/it]Training:   2%|▏         | 9/459 [00:19<19:10,  2.56s/it]Training:   2%|▏         | 10/459 [00:21<18:32,  2.48s/it]Training:   2%|▏         | 11/459 [00:23<17:02,  2.28s/it]Training:   3%|▎         | 12/459 [00:25<16:08,  2.17s/it]Training:   3%|▎         | 13/459 [00:28<16:44,  2.25s/it]Training:   3%|▎         | 14/459 [00:30<16:02,  2.16s/it]Training:   3%|▎         | 15/459 [00:32<15:57,  2.16s/it]Training:   3%|▎         | 16/459 [00:33<14:57,  2.03s/it]Training:   4%|▎         | 17/459 [00:35<14:13,  1.93s/it]Training:   4%|▍         | 18/459 [00:37<13:14,  1.80s/it]Training:   4%|▍         | 19/459 [00:38<12:28,  1.70s/it]Training:   4%|▍         | 20/459 [00:40<13:28,  1.84s/it]Training:   5%|▍         | 21/459 [00:42<13:28,  1.85s/it]Training:   5%|▍         | 22/459 [00:44<12:23,  1.70s/it]Training:   5%|▌         | 23/459 [00:45<12:15,  1.69s/it]Training:   5%|▌         | 24/459 [00:47<12:25,  1.71s/it]Training:   5%|▌         | 25/459 [00:49<13:45,  1.90s/it]Training:   6%|▌         | 26/459 [00:51<13:36,  1.89s/it]Training:   6%|▌         | 27/459 [00:54<14:33,  2.02s/it]Training:   6%|▌         | 28/459 [00:59<21:41,  3.02s/it]Training:   6%|▋         | 29/459 [01:01<19:07,  2.67s/it]Training:   7%|▋         | 30/459 [01:02<17:10,  2.40s/it]Training:   7%|▋         | 31/459 [01:04<15:31,  2.18s/it]Training:   7%|▋         | 32/459 [01:09<20:41,  2.91s/it]Training:   7%|▋         | 33/459 [01:10<17:28,  2.46s/it]Training:   7%|▋         | 34/459 [01:12<16:22,  2.31s/it]Training:   8%|▊         | 35/459 [01:14<16:24,  2.32s/it]Training:   8%|▊         | 36/459 [01:17<16:07,  2.29s/it]Training:   8%|▊         | 37/459 [01:18<13:46,  1.96s/it]Training:   8%|▊         | 38/459 [01:20<13:11,  1.88s/it]Training:   8%|▊         | 39/459 [01:21<12:45,  1.82s/it]Training:   9%|▊         | 40/459 [01:24<13:49,  1.98s/it]Training:   9%|▉         | 41/459 [01:26<14:52,  2.13s/it]Training:   9%|▉         | 42/459 [01:27<13:13,  1.90s/it]Training:   9%|▉         | 43/459 [01:32<17:51,  2.58s/it]Training:  10%|▉         | 44/459 [01:34<17:10,  2.48s/it]Training:  10%|▉         | 45/459 [01:37<18:11,  2.64s/it]Training:  10%|█         | 46/459 [01:39<16:26,  2.39s/it]Training:  10%|█         | 47/459 [01:41<15:36,  2.27s/it]Training:  10%|█         | 48/459 [01:42<13:40,  2.00s/it]Training:  11%|█         | 49/459 [01:44<13:36,  1.99s/it]Training:  11%|█         | 50/459 [01:46<13:58,  2.05s/it]Training:  11%|█         | 51/459 [01:48<12:51,  1.89s/it]Training:  11%|█▏        | 52/459 [01:49<12:22,  1.82s/it]Training:  12%|█▏        | 53/459 [01:52<13:02,  1.93s/it]Training:  12%|█▏        | 54/459 [01:53<12:48,  1.90s/it]Training:  12%|█▏        | 55/459 [01:55<12:20,  1.83s/it]Training:  12%|█▏        | 56/459 [01:57<11:40,  1.74s/it]Training:  12%|█▏        | 57/459 [01:58<10:48,  1.61s/it]Training:  13%|█▎        | 58/459 [01:59<10:10,  1.52s/it]Training:  13%|█▎        | 59/459 [02:02<12:30,  1.88s/it]Training:  13%|█▎        | 60/459 [02:03<11:47,  1.77s/it]Training:  13%|█▎        | 61/459 [02:05<11:34,  1.74s/it]Training:  14%|█▎        | 62/459 [02:07<11:02,  1.67s/it]Training:  14%|█▎        | 63/459 [02:09<12:04,  1.83s/it]/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/functional.py:1333: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return _VF.tensordot(a, b, dims_a, dims_b)  # type: ignore[attr-defined]
Training:  14%|█▎        | 63/459 [03:20<21:02,  3.19s/it]
Traceback (most recent call last):
  File "/home/vqchen_umass_edu/dialogue-kt/soap.py", line 361, in get_orthogonal_matrix
    _, Q = torch.linalg.eigh(m+1e-30*torch.eye(m.shape[0], device=m.device))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 173.25 MiB is free. Including non-PyTorch memory, this process has 47.10 GiB memory in use. Of the allocated memory 46.49 GiB is allocated by PyTorch, and 371.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/main.py", line 71, in <module>
    main()
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/main.py", line 68, in main
    args.func(args)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 129, in train
    return crossval(args, fn)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 94, in crossval
    metrics = fn(args, fold)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 272, in train_lmkt
    optimizer.step()
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/vqchen_umass_edu/dialogue-kt/soap.py", line 149, in step
    self.update_preconditioner(grad, state,
  File "/home/vqchen_umass_edu/dialogue-kt/soap.py", line 301, in update_preconditioner
    state['Q'] = self.get_orthogonal_matrix(state['GG'])
  File "/home/vqchen_umass_edu/dialogue-kt/soap.py", line 363, in get_orthogonal_matrix
    _, Q = torch.linalg.eigh(m.to(torch.float64)+1e-30*torch.eye(m.shape[0], device=m.device))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 518.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 45.25 MiB is free. Including non-PyTorch memory, this process has 47.22 GiB memory in use. Of the allocated memory 46.61 GiB is allocated by PyTorch, and 371.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
