No conda package cache directories found outside your home directory. To
prevent conda from filling up your home directory, you can create a new
directory at `/work/pi_<your_pi_name>/$USER/.conda/pkgs` and reload the module. 
No conda environment directories found outside your home directory. To prevent
conda from filling up your home directory, you can create a new directory at
`/work/pi_<your_pi_name>/$USER/.conda/envs` and reload the module. 
Loading conda version miniforge3-24.7.1
Loading cuda version 12.6
Loading cudnn version 8.9.7.29-12
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:24,  8.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:18,  9.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:27<00:09,  9.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  5.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  7.00s/it]
Training:   0%|          | 0/4936 [00:00<?, ?it/s]/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/backends/default/ops.py:68: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  output = output.addmm(subA, subB)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/nn/modules/linear.py:134: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return F.linear(input, self.weight, self.bias)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:290: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  grad_A = torch.matmul(grad_output.to(ctx.dtype_A), CB).view(ctx.grad_shape)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/autograd/graph.py:841: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/autograd/graph.py:841: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:897.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Training:   0%|          | 1/4936 [00:02<2:49:26,  2.06s/it]Training:   0%|          | 2/4936 [00:03<2:33:31,  1.87s/it]Training:   0%|          | 3/4936 [00:05<2:26:28,  1.78s/it]Training:   0%|          | 4/4936 [00:07<2:20:06,  1.70s/it]Training:   0%|          | 5/4936 [00:08<2:19:03,  1.69s/it]Training:   0%|          | 6/4936 [00:10<2:26:25,  1.78s/it]Training:   0%|          | 7/4936 [00:12<2:22:36,  1.74s/it]Training:   0%|          | 8/4936 [00:14<2:34:09,  1.88s/it]Training:   0%|          | 9/4936 [00:16<2:27:07,  1.79s/it]Training:   0%|          | 10/4936 [00:18<2:39:47,  1.95s/it]Training:   0%|          | 11/4936 [00:19<2:24:17,  1.76s/it]Training:   0%|          | 12/4936 [00:23<3:02:16,  2.22s/it]Training:   0%|          | 13/4936 [00:25<3:02:55,  2.23s/it]Training:   0%|          | 14/4936 [00:27<3:02:28,  2.22s/it]Training:   0%|          | 15/4936 [00:29<2:47:25,  2.04s/it]Training:   0%|          | 16/4936 [00:30<2:36:52,  1.91s/it]Training:   0%|          | 17/4936 [00:32<2:34:00,  1.88s/it]Training:   0%|          | 18/4936 [00:34<2:32:28,  1.86s/it]Training:   0%|          | 19/4936 [00:36<2:28:08,  1.81s/it]Training:   0%|          | 20/4936 [00:37<2:26:52,  1.79s/it]Training:   0%|          | 21/4936 [00:39<2:24:17,  1.76s/it]Training:   0%|          | 22/4936 [00:40<2:09:36,  1.58s/it]Training:   0%|          | 23/4936 [00:42<2:15:15,  1.65s/it]Training:   0%|          | 24/4936 [00:44<2:16:59,  1.67s/it]Training:   1%|          | 25/4936 [00:46<2:33:39,  1.88s/it]Training:   1%|          | 26/4936 [00:49<2:48:54,  2.06s/it]Training:   1%|          | 27/4936 [00:50<2:34:47,  1.89s/it]Training:   1%|          | 28/4936 [00:52<2:41:07,  1.97s/it]Training:   1%|          | 29/4936 [00:54<2:38:26,  1.94s/it]Training:   1%|          | 30/4936 [00:55<2:27:07,  1.80s/it]Training:   1%|          | 31/4936 [00:57<2:17:26,  1.68s/it]Training:   1%|          | 32/4936 [01:01<3:15:00,  2.39s/it]Training:   1%|          | 33/4936 [01:03<3:02:20,  2.23s/it]Training:   1%|          | 34/4936 [01:04<2:43:26,  2.00s/it]Training:   1%|          | 35/4936 [01:06<2:47:33,  2.05s/it]Training:   1%|          | 36/4936 [01:08<2:42:13,  1.99s/it]Training:   1%|          | 37/4936 [01:10<2:33:19,  1.88s/it]Training:   1%|          | 38/4936 [01:12<2:36:51,  1.92s/it]Training:   1%|          | 39/4936 [01:14<2:34:00,  1.89s/it]Training:   1%|          | 40/4936 [01:15<2:22:28,  1.75s/it]Training:   1%|          | 41/4936 [01:17<2:27:47,  1.81s/it]Training:   1%|          | 42/4936 [01:19<2:27:33,  1.81s/it]Training:   1%|          | 43/4936 [01:20<2:16:12,  1.67s/it]Training:   1%|          | 44/4936 [01:23<2:32:45,  1.87s/it]Training:   1%|          | 45/4936 [01:24<2:29:45,  1.84s/it]Training:   1%|          | 46/4936 [01:26<2:24:52,  1.78s/it]Training:   1%|          | 47/4936 [01:28<2:22:30,  1.75s/it]Training:   1%|          | 48/4936 [01:30<2:28:02,  1.82s/it]Training:   1%|          | 49/4936 [01:31<2:22:01,  1.74s/it]Training:   1%|          | 50/4936 [01:33<2:14:31,  1.65s/it]Training:   1%|          | 51/4936 [01:36<2:46:21,  2.04s/it]Training:   1%|          | 52/4936 [01:38<2:42:45,  2.00s/it]Training:   1%|          | 53/4936 [01:40<2:42:32,  2.00s/it]Training:   1%|          | 54/4936 [01:41<2:29:30,  1.84s/it]slurmstepd-gypsum-gpu182: error: *** JOB 50443278 ON gypsum-gpu182 CANCELLED AT 2025-12-12T00:24:23 ***
