No conda package cache directories found outside your home directory. To
prevent conda from filling up your home directory, you can create a new
directory at `/work/pi_<your_pi_name>/$USER/.conda/pkgs` and reload the module. 
No conda environment directories found outside your home directory. To prevent
conda from filling up your home directory, you can create a new directory at
`/work/pi_<your_pi_name>/$USER/.conda/envs` and reload the module. 
Loading conda version miniforge3-24.7.1
Loading cuda version 12.6
Loading cudnn version 8.9.7.29-12
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.67s/it]
Training:   0%|          | 0/4936 [00:00<?, ?it/s]/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:209: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/nn/modules/linear.py:134: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return F.linear(input, self.weight, self.bias)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:660: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/autograd/graph.py:841: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Training:   0%|          | 1/4936 [00:03<5:26:36,  3.97s/it]Training:   0%|          | 2/4936 [00:06<4:05:57,  2.99s/it]Training:   0%|          | 3/4936 [00:08<3:37:42,  2.65s/it]Training:   0%|          | 4/4936 [00:10<3:16:16,  2.39s/it]Training:   0%|          | 5/4936 [00:12<3:10:25,  2.32s/it]Training:   0%|          | 6/4936 [00:15<3:23:05,  2.47s/it]Training:   0%|          | 7/4936 [00:17<3:13:35,  2.36s/it]Training:   0%|          | 8/4936 [00:20<3:33:38,  2.60s/it]Training:   0%|          | 9/4936 [00:22<3:20:19,  2.44s/it]Training:   0%|          | 10/4936 [00:26<3:42:21,  2.71s/it]Training:   0%|          | 11/4936 [00:27<3:15:56,  2.39s/it]Training:   0%|          | 12/4936 [00:32<4:15:54,  3.12s/it]Training:   0%|          | 13/4936 [00:35<4:22:26,  3.20s/it]Training:   0%|          | 14/4936 [00:39<4:23:49,  3.22s/it]Training:   0%|          | 15/4936 [00:41<3:56:23,  2.88s/it]Training:   0%|          | 16/4936 [00:43<3:36:35,  2.64s/it]Training:   0%|          | 17/4936 [00:45<3:33:15,  2.60s/it]Training:   0%|          | 18/4936 [00:48<3:31:42,  2.58s/it]Training:   0%|          | 19/4936 [00:50<3:24:33,  2.50s/it]Training:   0%|          | 20/4936 [00:53<3:22:05,  2.47s/it]Training:   0%|          | 21/4936 [00:55<3:17:35,  2.41s/it]Training:   0%|          | 22/4936 [00:56<2:49:56,  2.08s/it]Training:   0%|          | 23/4936 [00:59<2:59:02,  2.19s/it]Training:   0%|          | 24/4936 [01:01<3:04:39,  2.26s/it]Training:   1%|          | 25/4936 [01:04<3:32:56,  2.60s/it]Training:   1%|          | 26/4936 [01:08<4:01:20,  2.95s/it]Training:   1%|          | 27/4936 [01:10<3:37:42,  2.66s/it]Training:   1%|          | 28/4936 [01:13<3:48:07,  2.79s/it]Training:   1%|          | 29/4936 [01:16<3:45:12,  2.75s/it]Training:   1%|          | 30/4936 [01:18<3:24:37,  2.50s/it]Training:   1%|          | 31/4936 [01:20<3:06:48,  2.29s/it]Training:   1%|          | 32/4936 [01:26<4:37:25,  3.39s/it]Training:   1%|          | 33/4936 [01:28<4:23:31,  3.22s/it]Training:   1%|          | 34/4936 [01:30<3:50:58,  2.83s/it]Training:   1%|          | 35/4936 [01:33<3:57:50,  2.91s/it]Training:   1%|          | 36/4936 [01:36<3:50:42,  2.83s/it]Training:   1%|          | 37/4936 [01:38<3:33:59,  2.62s/it]Training:   1%|          | 38/4936 [01:41<3:41:09,  2.71s/it]Training:   1%|          | 39/4936 [01:44<3:38:11,  2.67s/it]Training:   1%|          | 40/4936 [01:46<3:17:36,  2.42s/it]Training:   1%|          | 41/4936 [01:48<3:26:47,  2.53s/it]Training:   1%|          | 42/4936 [01:51<3:28:03,  2.55s/it]Training:   1%|          | 43/4936 [01:53<3:05:27,  2.27s/it]Training:   1%|          | 44/4936 [01:56<3:31:57,  2.60s/it]Training:   1%|          | 45/4936 [01:58<3:29:10,  2.57s/it]Training:   1%|          | 46/4936 [02:01<3:18:46,  2.44s/it]Training:   1%|          | 47/4936 [02:03<3:14:48,  2.39s/it]Training:   1%|          | 48/4936 [02:06<3:25:31,  2.52s/it]Training:   1%|          | 49/4936 [02:08<3:14:38,  2.39s/it]Training:   1%|          | 50/4936 [02:10<3:01:19,  2.23s/it]Training:   1%|          | 51/4936 [02:14<3:52:00,  2.85s/it]Training:   1%|          | 52/4936 [02:17<3:50:24,  2.83s/it]Training:   1%|          | 53/4936 [02:20<3:51:48,  2.85s/it]Training:   1%|          | 54/4936 [02:22<3:29:21,  2.57s/it]Training:   1%|          | 55/4936 [02:24<3:24:38,  2.52s/it]Training:   1%|          | 56/4936 [02:27<3:39:54,  2.70s/it]Training:   1%|          | 57/4936 [02:30<3:38:12,  2.68s/it]Training:   1%|          | 58/4936 [02:33<4:04:43,  3.01s/it]Training:   1%|          | 59/4936 [02:41<6:02:55,  4.46s/it]Training:   1%|          | 60/4936 [02:44<5:22:08,  3.96s/it]Training:   1%|          | 61/4936 [02:47<5:04:05,  3.74s/it]Training:   1%|▏         | 62/4936 [02:50<4:37:58,  3.42s/it]Training:   1%|▏         | 63/4936 [02:56<5:27:50,  4.04s/it]Training:   1%|▏         | 64/4936 [03:00<5:44:16,  4.24s/it]Training:   1%|▏         | 65/4936 [03:02<4:46:40,  3.53s/it]Training:   1%|▏         | 66/4936 [03:05<4:22:30,  3.23s/it]Training:   1%|▏         | 67/4936 [03:08<4:13:50,  3.13s/it]Training:   1%|▏         | 68/4936 [03:09<3:37:57,  2.69s/it]Training:   1%|▏         | 69/4936 [03:11<3:20:49,  2.48s/it]Training:   1%|▏         | 70/4936 [03:13<3:15:49,  2.41s/it]Training:   1%|▏         | 71/4936 [03:17<3:47:04,  2.80s/it]Training:   1%|▏         | 72/4936 [03:20<3:40:19,  2.72s/it]Training:   1%|▏         | 73/4936 [03:22<3:33:07,  2.63s/it]Training:   1%|▏         | 74/4936 [03:24<3:15:23,  2.41s/it]Training:   2%|▏         | 75/4936 [03:25<2:53:15,  2.14s/it]Training:   2%|▏         | 76/4936 [03:28<3:08:58,  2.33s/it]Training:   2%|▏         | 77/4936 [03:30<3:03:26,  2.27s/it]Training:   2%|▏         | 78/4936 [03:33<3:09:58,  2.35s/it]Training:   2%|▏         | 79/4936 [03:36<3:15:41,  2.42s/it]Training:   2%|▏         | 80/4936 [03:39<3:33:44,  2.64s/it]Training:   2%|▏         | 81/4936 [03:40<3:10:02,  2.35s/it]Training:   2%|▏         | 82/4936 [03:46<4:38:29,  3.44s/it]Training:   2%|▏         | 83/4936 [03:48<4:00:48,  2.98s/it]Training:   2%|▏         | 84/4936 [03:51<3:56:39,  2.93s/it]Training:   2%|▏         | 85/4936 [03:53<3:42:49,  2.76s/it]Training:   2%|▏         | 86/4936 [03:57<4:07:03,  3.06s/it]Training:   2%|▏         | 87/4936 [03:59<3:45:58,  2.80s/it]Training:   2%|▏         | 88/4936 [04:03<4:08:38,  3.08s/it]Training:   2%|▏         | 89/4936 [04:05<3:42:25,  2.75s/it]Training:   2%|▏         | 90/4936 [04:09<4:04:43,  3.03s/it]Training:   2%|▏         | 91/4936 [04:11<3:53:00,  2.89s/it]Training:   2%|▏         | 92/4936 [04:14<3:52:48,  2.88s/it]Training:   2%|▏         | 93/4936 [04:17<4:01:40,  2.99s/it]Training:   2%|▏         | 94/4936 [04:20<3:52:58,  2.89s/it]Training:   2%|▏         | 95/4936 [04:22<3:34:41,  2.66s/it]Training:   2%|▏         | 96/4936 [04:25<3:31:46,  2.63s/it]Training:   2%|▏         | 97/4936 [04:27<3:18:14,  2.46s/it]Training:   2%|▏         | 98/4936 [04:29<3:09:51,  2.35s/it]Training:   2%|▏         | 99/4936 [04:31<3:14:23,  2.41s/it]Training:   2%|▏         | 100/4936 [04:36<4:01:36,  3.00s/it]Training:   2%|▏         | 101/4936 [04:38<3:48:37,  2.84s/it]Training:   2%|▏         | 102/4936 [04:40<3:31:18,  2.62s/it]Training:   2%|▏         | 103/4936 [04:44<3:43:43,  2.78s/it]Training:   2%|▏         | 104/4936 [04:45<3:16:49,  2.44s/it]Training:   2%|▏         | 105/4936 [04:48<3:19:23,  2.48s/it]Training:   2%|▏         | 106/4936 [04:50<3:09:56,  2.36s/it]Training:   2%|▏         | 107/4936 [04:51<2:51:04,  2.13s/it]Training:   2%|▏         | 108/4936 [04:55<3:22:18,  2.51s/it]Training:   2%|▏         | 109/4936 [04:57<3:03:02,  2.28s/it]Training:   2%|▏         | 110/4936 [04:58<2:52:23,  2.14s/it]Training:   2%|▏         | 111/4936 [05:04<4:11:46,  3.13s/it]Training:   2%|▏         | 112/4936 [05:06<3:51:04,  2.87s/it]Training:   2%|▏         | 113/4936 [05:09<3:56:32,  2.94s/it]Training:   2%|▏         | 114/4936 [05:12<3:48:57,  2.85s/it]Training:   2%|▏         | 115/4936 [05:15<3:49:28,  2.86s/it]Training:   2%|▏         | 116/4936 [05:17<3:32:41,  2.65s/it]Training:   2%|▏         | 117/4936 [05:19<3:26:53,  2.58s/it]Training:   2%|▏         | 118/4936 [05:21<3:14:11,  2.42s/it]Training:   2%|▏         | 119/4936 [05:24<3:26:18,  2.57s/it]Training:   2%|▏         | 120/4936 [05:26<3:11:09,  2.38s/it]Training:   2%|▏         | 121/4936 [05:29<3:15:49,  2.44s/it]Training:   2%|▏         | 122/4936 [05:31<3:16:12,  2.45s/it]Training:   2%|▏         | 123/4936 [05:33<3:01:22,  2.26s/it]Training:   3%|▎         | 124/4936 [05:35<2:56:51,  2.21s/it]Training:   3%|▎         | 125/4936 [05:38<3:01:55,  2.27s/it]Training:   3%|▎         | 126/4936 [05:39<2:51:19,  2.14s/it]Training:   3%|▎         | 127/4936 [05:42<3:14:30,  2.43s/it]Training:   3%|▎         | 128/4936 [05:46<3:45:42,  2.82s/it]Training:   3%|▎         | 129/4936 [05:48<3:29:09,  2.61s/it]Training:   3%|▎         | 130/4936 [05:51<3:41:51,  2.77s/it]Training:   3%|▎         | 131/4936 [05:54<3:38:18,  2.73s/it]Training:   3%|▎         | 132/4936 [05:56<3:11:42,  2.39s/it]Training:   3%|▎         | 133/4936 [05:57<2:51:34,  2.14s/it]Training:   3%|▎         | 134/4936 [05:59<2:50:01,  2.12s/it]Training:   3%|▎         | 135/4936 [06:07<5:07:58,  3.85s/it]Training:   3%|▎         | 136/4936 [06:09<4:17:13,  3.22s/it]Training:   3%|▎         | 137/4936 [06:11<3:43:39,  2.80s/it]Training:   3%|▎         | 138/4936 [06:13<3:31:00,  2.64s/it]Training:   3%|▎         | 139/4936 [06:17<3:56:16,  2.96s/it]Training:   3%|▎         | 140/4936 [06:19<3:31:41,  2.65s/it]Training:   3%|▎         | 141/4936 [06:21<3:22:19,  2.53s/it]Training:   3%|▎         | 142/4936 [06:23<2:59:45,  2.25s/it]Training:   3%|▎         | 143/4936 [06:24<2:42:52,  2.04s/it]Training:   3%|▎         | 144/4936 [06:27<3:01:24,  2.27s/it]Training:   3%|▎         | 145/4936 [06:29<2:53:08,  2.17s/it]Training:   3%|▎         | 145/4936 [06:33<3:36:54,  2.72s/it]
Traceback (most recent call last):
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/main.py", line 71, in <module>
    main()
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/main.py", line 68, in main
    args.func(args)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 127, in train
    return crossval(args, fn)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 92, in crossval
    metrics = fn(args, fold)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 262, in train_lmkt
    loss, _, _ = get_loss(model, batch, true_token, false_token, args)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 204, in get_lmkt_loss_packed
    model_output = model(input_ids=batch["input_ids"], attention_mask=attention_mask, position_ids=batch["position_ids"])
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/peft_model.py", line 1577, in forward
    return self.base_model(
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1209, in forward
    logits = logits.float()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 826.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 261.25 MiB is free. Including non-PyTorch memory, this process has 47.01 GiB memory in use. Of the allocated memory 45.15 GiB is allocated by PyTorch, and 1.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
