Removing conda version miniforge3-24.7.1
No conda package cache directories found outside your home directory. To
prevent conda from filling up your home directory, you can create a new
directory at `/work/pi_<your_pi_name>/$USER/.conda/pkgs` and reload the module. 
No conda environment directories found outside your home directory. To prevent
conda from filling up your home directory, you can create a new directory at
`/work/pi_<your_pi_name>/$USER/.conda/envs` and reload the module. 
Loading conda version miniforge3-24.7.1
Loading cuda version 12.6
Loading cudnn version 8.9.7.29-12
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:45, 15.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:28<00:28, 14.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:13, 13.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00,  9.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00, 11.28s/it]
Traceback (most recent call last):
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/main.py", line 71, in <module>
    main()
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/main.py", line 68, in main
    args.func(args)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 127, in train
    return crossval(args, fn)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 92, in crossval
    metrics = fn(args, fold)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 227, in train_lmkt
    model, tokenizer = get_model(args.base_model, False, pt_model_name=args.pt_model_name, r=args.r, lora_alpha=args.lora_alpha, quantize=args.quantize)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/models/lm.py", line 32, in get_model
    model = get_base_model(base_model_name, tokenizer, quantize)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/models/lm.py", line 13, in get_base_model
    base_model = AutoModelForCausalLM.from_pretrained(
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4015, in from_pretrained
    dispatch_model(model, **device_map_kwargs)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/accelerate/big_modeling.py", line 502, in dispatch_model
    model.to(device)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2861, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
