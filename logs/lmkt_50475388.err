No conda package cache directories found outside your home directory. To
prevent conda from filling up your home directory, you can create a new
directory at `/work/pi_<your_pi_name>/$USER/.conda/pkgs` and reload the module. 
No conda environment directories found outside your home directory. To prevent
conda from filling up your home directory, you can create a new directory at
`/work/pi_<your_pi_name>/$USER/.conda/envs` and reload the module. 
Loading conda version miniforge3-24.7.1
Loading cuda version 12.6
Loading cudnn version 8.9.7.29-12
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:23,  7.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.86s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:09,  9.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  5.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.86s/it]
Training:   0%|          | 0/4936 [00:00<?, ?it/s]/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/backends/default/ops.py:68: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  output = output.addmm(subA, subB)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/nn/modules/linear.py:134: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return F.linear(input, self.weight, self.bias)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:290: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  grad_A = torch.matmul(grad_output.to(ctx.dtype_A), CB).view(ctx.grad_shape)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/autograd/graph.py:841: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/autograd/graph.py:841: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:897.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Training:   0%|          | 1/4936 [00:01<2:41:22,  1.96s/it]Training:   0%|          | 2/4936 [00:03<2:31:00,  1.84s/it]Training:   0%|          | 3/4936 [00:05<2:26:33,  1.78s/it]Training:   0%|          | 4/4936 [00:07<2:20:32,  1.71s/it]Training:   0%|          | 5/4936 [00:08<2:20:38,  1.71s/it]Training:   0%|          | 6/4936 [00:10<2:29:06,  1.81s/it]Training:   0%|          | 7/4936 [00:12<2:24:52,  1.76s/it]Training:   0%|          | 8/4936 [00:14<2:37:36,  1.92s/it]Training:   0%|          | 9/4936 [00:16<2:29:38,  1.82s/it]Training:   0%|          | 10/4936 [00:18<2:43:36,  1.99s/it]Training:   0%|          | 11/4936 [00:19<2:26:07,  1.78s/it]Training:   0%|          | 12/4936 [00:23<3:06:51,  2.28s/it]Training:   0%|          | 13/4936 [00:25<3:07:37,  2.29s/it]Training:   0%|          | 14/4936 [00:27<3:07:44,  2.29s/it]Training:   0%|          | 15/4936 [00:29<2:51:26,  2.09s/it]Training:   0%|          | 16/4936 [00:31<2:41:17,  1.97s/it]Training:   0%|          | 17/4936 [00:33<2:38:20,  1.93s/it]Training:   0%|          | 18/4936 [00:34<2:36:45,  1.91s/it]Training:   0%|          | 19/4936 [00:36<2:32:50,  1.86s/it]Training:   0%|          | 20/4936 [00:38<2:30:25,  1.84s/it]Training:   0%|          | 21/4936 [00:40<2:28:34,  1.81s/it]Training:   0%|          | 22/4936 [00:41<2:12:11,  1.61s/it]Training:   0%|          | 23/4936 [00:43<2:17:57,  1.68s/it]Training:   0%|          | 24/4936 [00:45<2:20:41,  1.72s/it]Training:   1%|          | 25/4936 [00:47<2:37:46,  1.93s/it]Training:   1%|          | 26/4936 [00:50<2:54:24,  2.13s/it]Training:   1%|          | 27/4936 [00:51<2:38:29,  1.94s/it]Training:   1%|          | 28/4936 [00:53<2:46:02,  2.03s/it]Training:   1%|          | 29/4936 [00:55<2:43:32,  2.00s/it]Training:   1%|          | 30/4936 [00:57<2:31:07,  1.85s/it]Training:   1%|          | 31/4936 [00:58<2:20:49,  1.72s/it]Training:   1%|          | 32/4936 [01:02<3:21:57,  2.47s/it]Training:   1%|          | 33/4936 [01:04<3:08:36,  2.31s/it]Training:   1%|          | 34/4936 [01:06<2:48:21,  2.06s/it]Training:   1%|          | 35/4936 [01:08<2:53:30,  2.12s/it]Training:   1%|          | 36/4936 [01:10<2:47:35,  2.05s/it]Training:   1%|          | 37/4936 [01:12<2:38:08,  1.94s/it]Training:   1%|          | 38/4936 [01:14<2:42:04,  1.99s/it]Training:   1%|          | 39/4936 [01:16<2:39:24,  1.95s/it]Training:   1%|          | 40/4936 [01:17<2:26:51,  1.80s/it]Training:   1%|          | 41/4936 [01:19<2:32:53,  1.87s/it]Training:   1%|          | 42/4936 [01:21<2:33:10,  1.88s/it]Training:   1%|          | 43/4936 [01:22<2:19:34,  1.71s/it]Training:   1%|          | 44/4936 [01:25<2:37:34,  1.93s/it]Training:   1%|          | 45/4936 [01:27<2:34:28,  1.89s/it]Training:   1%|          | 46/4936 [01:28<2:29:00,  1.83s/it]Training:   1%|          | 47/4936 [01:30<2:26:13,  1.79s/it]Training:   1%|          | 48/4936 [01:32<2:32:33,  1.87s/it]Training:   1%|          | 49/4936 [01:34<2:26:19,  1.80s/it]Training:   1%|          | 50/4936 [01:35<2:18:07,  1.70s/it]Training:   1%|          | 51/4936 [01:38<2:52:18,  2.12s/it]Training:   1%|          | 52/4936 [01:40<2:48:48,  2.07s/it]Training:   1%|          | 53/4936 [01:42<2:49:10,  2.08s/it]Training:   1%|          | 54/4936 [01:44<2:35:02,  1.91s/it]Training:   1%|          | 55/4936 [01:46<2:32:03,  1.87s/it]Training:   1%|          | 56/4936 [01:48<2:42:08,  1.99s/it]Training:   1%|          | 57/4936 [01:50<2:39:59,  1.97s/it]Training:   1%|          | 58/4936 [01:52<2:57:21,  2.18s/it]Training:   1%|          | 59/4936 [01:58<4:13:27,  3.12s/it]Training:   1%|          | 60/4936 [02:00<3:42:42,  2.74s/it]Training:   1%|          | 61/4936 [02:02<3:32:39,  2.62s/it]Training:   1%|▏         | 62/4936 [02:04<3:15:59,  2.41s/it]Training:   1%|▏         | 63/4936 [02:08<3:51:23,  2.85s/it]Training:   1%|▏         | 64/4936 [02:11<3:59:27,  2.95s/it]Training:   1%|▏         | 65/4936 [02:13<3:27:08,  2.55s/it]Training:   1%|▏         | 66/4936 [02:14<3:11:14,  2.36s/it]Training:   1%|▏         | 67/4936 [02:16<3:04:37,  2.28s/it]Training:   1%|▏         | 68/4936 [02:18<2:41:34,  1.99s/it]Training:   1%|▏         | 69/4936 [02:19<2:32:28,  1.88s/it]Training:   1%|▏         | 70/4936 [02:21<2:29:04,  1.84s/it]Training:   1%|▏         | 71/4936 [02:24<2:49:22,  2.09s/it]Training:   1%|▏         | 72/4936 [02:26<2:42:24,  2.00s/it]Training:   1%|▏         | 73/4936 [02:27<2:37:28,  1.94s/it]Training:   1%|▏         | 74/4936 [02:29<2:26:32,  1.81s/it]Training:   2%|▏         | 75/4936 [02:30<2:13:38,  1.65s/it]Training:   2%|▏         | 76/4936 [02:32<2:23:27,  1.77s/it]Training:   2%|▏         | 77/4936 [02:34<2:20:38,  1.74s/it]Training:   2%|▏         | 78/4936 [02:36<2:24:14,  1.78s/it]Training:   2%|▏         | 79/4936 [02:38<2:26:37,  1.81s/it]Training:   2%|▏         | 80/4936 [02:40<2:38:23,  1.96s/it]Training:   2%|▏         | 81/4936 [02:41<2:22:58,  1.77s/it]Training:   2%|▏         | 82/4936 [02:46<3:23:43,  2.52s/it]Training:   2%|▏         | 83/4936 [02:47<2:55:51,  2.17s/it]Training:   2%|▏         | 84/4936 [02:49<2:53:34,  2.15s/it]Training:   2%|▏         | 85/4936 [02:51<2:44:23,  2.03s/it]Training:   2%|▏         | 86/4936 [02:54<3:00:31,  2.23s/it]Training:   2%|▏         | 87/4936 [02:55<2:46:47,  2.06s/it]Training:   2%|▏         | 88/4936 [02:58<3:01:19,  2.24s/it]Training:   2%|▏         | 89/4936 [02:59<2:43:03,  2.02s/it]Training:   2%|▏         | 90/4936 [03:02<2:58:26,  2.21s/it]Training:   2%|▏         | 91/4936 [03:04<2:48:54,  2.09s/it]Training:   2%|▏         | 92/4936 [03:06<2:48:51,  2.09s/it]Training:   2%|▏         | 93/4936 [03:08<2:54:41,  2.16s/it]Training:   2%|▏         | 94/4936 [03:10<2:48:17,  2.09s/it]Training:   2%|▏         | 95/4936 [03:12<2:37:53,  1.96s/it]Training:   2%|▏         | 96/4936 [03:14<2:36:16,  1.94s/it]Training:   2%|▏         | 97/4936 [03:15<2:29:03,  1.85s/it]Training:   2%|▏         | 98/4936 [03:17<2:24:52,  1.80s/it]Training:   2%|▏         | 99/4936 [03:19<2:27:04,  1.82s/it]Training:   2%|▏         | 100/4936 [03:22<2:58:18,  2.21s/it]Training:   2%|▏         | 101/4936 [03:24<2:47:39,  2.08s/it]Training:   2%|▏         | 102/4936 [03:25<2:37:47,  1.96s/it]Training:   2%|▏         | 103/4936 [03:28<2:45:57,  2.06s/it]Training:   2%|▏         | 104/4936 [03:29<2:28:04,  1.84s/it]Training:   2%|▏         | 105/4936 [03:31<2:30:01,  1.86s/it]Training:   2%|▏         | 106/4936 [03:33<2:24:46,  1.80s/it]Training:   2%|▏         | 107/4936 [03:34<2:13:05,  1.65s/it]Training:   2%|▏         | 108/4936 [03:36<2:33:26,  1.91s/it]Training:   2%|▏         | 109/4936 [03:38<2:19:49,  1.74s/it]Training:   2%|▏         | 110/4936 [03:39<2:13:50,  1.66s/it]Training:   2%|▏         | 111/4936 [03:43<3:06:59,  2.33s/it]Training:   2%|▏         | 112/4936 [03:45<2:51:05,  2.13s/it]Training:   2%|▏         | 113/4936 [03:47<2:54:53,  2.18s/it]Training:   2%|▏         | 114/4936 [03:49<2:48:30,  2.10s/it]Training:   2%|▏         | 115/4936 [03:51<2:47:56,  2.09s/it]Training:   2%|▏         | 116/4936 [03:53<2:37:53,  1.97s/it]Training:   2%|▏         | 117/4936 [03:55<2:33:50,  1.92s/it]Training:   2%|▏         | 118/4936 [03:56<2:26:47,  1.83s/it]Training:   2%|▏         | 119/4936 [03:58<2:34:23,  1.92s/it]Training:   2%|▏         | 120/4936 [04:00<2:24:05,  1.80s/it]Training:   2%|▏         | 121/4936 [04:02<2:26:59,  1.83s/it]Training:   2%|▏         | 122/4936 [04:04<2:26:19,  1.82s/it]Training:   2%|▏         | 123/4936 [04:05<2:17:16,  1.71s/it]Training:   3%|▎         | 124/4936 [04:07<2:15:55,  1.69s/it]Training:   3%|▎         | 125/4936 [04:08<2:18:43,  1.73s/it]Training:   3%|▎         | 126/4936 [04:10<2:11:55,  1.65s/it]Training:   3%|▎         | 127/4936 [04:12<2:27:11,  1.84s/it]Training:   3%|▎         | 128/4936 [04:15<2:42:54,  2.03s/it]Training:   3%|▎         | 129/4936 [04:16<2:35:33,  1.94s/it]Training:   3%|▎         | 130/4936 [04:19<2:43:57,  2.05s/it]Training:   3%|▎         | 131/4936 [04:21<2:40:28,  2.00s/it]Training:   3%|▎         | 132/4936 [04:22<2:23:41,  1.79s/it]Training:   3%|▎         | 133/4936 [04:23<2:12:05,  1.65s/it]Training:   3%|▎         | 134/4936 [04:25<2:13:03,  1.66s/it]Training:   3%|▎         | 135/4936 [04:30<3:43:30,  2.79s/it]Training:   3%|▎         | 136/4936 [04:32<3:06:26,  2.33s/it]Training:   3%|▎         | 137/4936 [04:33<2:46:09,  2.08s/it]Training:   3%|▎         | 138/4936 [04:35<2:38:41,  1.98s/it]Training:   3%|▎         | 139/4936 [04:38<2:54:36,  2.18s/it]Training:   3%|▎         | 140/4936 [04:39<2:37:11,  1.97s/it]Training:   3%|▎         | 141/4936 [04:41<2:32:08,  1.90s/it]Training:   3%|▎         | 142/4936 [04:42<2:17:40,  1.72s/it]Training:   3%|▎         | 143/4936 [04:43<2:07:31,  1.60s/it]Training:   3%|▎         | 144/4936 [04:45<2:19:09,  1.74s/it]Training:   3%|▎         | 145/4936 [04:47<2:13:12,  1.67s/it]Training:   3%|▎         | 146/4936 [04:53<3:49:34,  2.88s/it]Training:   3%|▎         | 147/4936 [04:54<3:18:06,  2.48s/it]Training:   3%|▎         | 148/4936 [04:56<2:50:10,  2.13s/it]Training:   3%|▎         | 149/4936 [04:57<2:34:39,  1.94s/it]Training:   3%|▎         | 150/4936 [04:59<2:34:39,  1.94s/it]Training:   3%|▎         | 151/4936 [05:01<2:31:01,  1.89s/it]Training:   3%|▎         | 152/4936 [05:02<2:20:32,  1.76s/it]Training:   3%|▎         | 153/4936 [05:04<2:18:19,  1.74s/it]Training:   3%|▎         | 154/4936 [05:06<2:19:09,  1.75s/it]Training:   3%|▎         | 155/4936 [05:07<2:17:22,  1.72s/it]Training:   3%|▎         | 156/4936 [05:09<2:11:35,  1.65s/it]Training:   3%|▎         | 157/4936 [05:11<2:17:20,  1.72s/it]Training:   3%|▎         | 158/4936 [05:12<2:16:10,  1.71s/it]Training:   3%|▎         | 159/4936 [05:14<2:21:17,  1.77s/it]Training:   3%|▎         | 160/4936 [05:17<2:43:08,  2.05s/it]Training:   3%|▎         | 161/4936 [05:19<2:43:57,  2.06s/it]Training:   3%|▎         | 162/4936 [05:20<2:26:08,  1.84s/it]Training:   3%|▎         | 163/4936 [05:22<2:12:54,  1.67s/it]Training:   3%|▎         | 164/4936 [05:24<2:28:15,  1.86s/it]Training:   3%|▎         | 165/4936 [05:26<2:27:13,  1.85s/it]Training:   3%|▎         | 166/4936 [05:28<2:25:06,  1.83s/it]Training:   3%|▎         | 167/4936 [05:29<2:16:29,  1.72s/it]Training:   3%|▎         | 168/4936 [05:31<2:24:47,  1.82s/it]Training:   3%|▎         | 169/4936 [05:33<2:37:16,  1.98s/it]Training:   3%|▎         | 170/4936 [05:36<2:45:03,  2.08s/it]Training:   3%|▎         | 171/4936 [05:38<2:41:01,  2.03s/it]Training:   3%|▎         | 172/4936 [05:39<2:27:23,  1.86s/it]Training:   4%|▎         | 173/4936 [05:41<2:18:04,  1.74s/it]Training:   4%|▎         | 174/4936 [05:42<2:08:00,  1.61s/it]Training:   4%|▎         | 175/4936 [05:44<2:12:07,  1.67s/it]Training:   4%|▎         | 176/4936 [05:45<2:03:55,  1.56s/it]Training:   4%|▎         | 177/4936 [05:47<2:05:34,  1.58s/it]Training:   4%|▎         | 178/4936 [05:49<2:17:18,  1.73s/it]Training:   4%|▎         | 179/4936 [05:50<2:07:06,  1.60s/it]Training:   4%|▎         | 180/4936 [05:51<2:04:03,  1.57s/it]Training:   4%|▎         | 181/4936 [05:53<2:10:26,  1.65s/it]Training:   4%|▎         | 182/4936 [05:55<2:06:20,  1.59s/it]Training:   4%|▎         | 183/4936 [05:56<2:03:42,  1.56s/it]Training:   4%|▎         | 184/4936 [05:58<2:06:27,  1.60s/it]Training:   4%|▎         | 185/4936 [05:59<1:59:54,  1.51s/it]Training:   4%|▍         | 186/4936 [06:01<2:05:39,  1.59s/it]Training:   4%|▍         | 187/4936 [06:05<3:00:01,  2.27s/it]Training:   4%|▍         | 188/4936 [06:07<2:56:10,  2.23s/it]Training:   4%|▍         | 189/4936 [06:09<2:48:45,  2.13s/it]Training:   4%|▍         | 190/4936 [06:11<2:40:46,  2.03s/it]Training:   4%|▍         | 191/4936 [06:14<3:01:32,  2.30s/it]Training:   4%|▍         | 192/4936 [06:16<2:57:54,  2.25s/it]Training:   4%|▍         | 193/4936 [06:17<2:42:55,  2.06s/it]Training:   4%|▍         | 194/4936 [06:19<2:39:37,  2.02s/it]Training:   4%|▍         | 195/4936 [06:21<2:30:42,  1.91s/it]Training:   4%|▍         | 196/4936 [06:22<2:19:53,  1.77s/it]Training:   4%|▍         | 197/4936 [06:24<2:09:04,  1.63s/it]Training:   4%|▍         | 198/4936 [06:28<3:03:34,  2.32s/it]Training:   4%|▍         | 199/4936 [06:30<2:51:28,  2.17s/it]slurmstepd-gypsum-gpu182: error: *** JOB 50475388 ON gypsum-gpu182 CANCELLED AT 2025-12-12T07:12:46 ***
