No conda package cache directories found outside your home directory. To
prevent conda from filling up your home directory, you can create a new
directory at `/work/pi_<your_pi_name>/$USER/.conda/pkgs` and reload the module. 
No conda environment directories found outside your home directory. To prevent
conda from filling up your home directory, you can create a new directory at
`/work/pi_<your_pi_name>/$USER/.conda/envs` and reload the module. 
Loading conda version miniforge3-24.7.1
Loading cuda version 12.6
Loading cudnn version 8.9.7.29-12
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.99s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  2.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.50s/it]
Training:   0%|          | 0/6567 [00:00<?, ?it/s]/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/backends/default/ops.py:68: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  output = output.addmm(subA, subB)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/nn/modules/linear.py:134: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return F.linear(input, self.weight, self.bias)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:290: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  grad_A = torch.matmul(grad_output.to(ctx.dtype_A), CB).view(ctx.grad_shape)
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/autograd/graph.py:841: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/torch/autograd/graph.py:841: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:897.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Training:   0%|          | 1/6567 [00:01<2:43:58,  1.50s/it]Training:   0%|          | 2/6567 [00:02<2:05:16,  1.15s/it]Training:   0%|          | 3/6567 [00:03<2:20:07,  1.28s/it]Training:   0%|          | 4/6567 [00:05<2:20:27,  1.28s/it]Training:   0%|          | 5/6567 [00:06<2:29:18,  1.37s/it]Training:   0%|          | 6/6567 [00:07<2:17:04,  1.25s/it]Training:   0%|          | 7/6567 [00:08<2:07:29,  1.17s/it]Training:   0%|          | 8/6567 [00:09<2:07:52,  1.17s/it]Training:   0%|          | 9/6567 [00:11<2:08:39,  1.18s/it]Training:   0%|          | 10/6567 [00:12<2:09:47,  1.19s/it]Training:   0%|          | 11/6567 [00:13<2:12:01,  1.21s/it]Training:   0%|          | 12/6567 [00:15<2:45:06,  1.51s/it]Training:   0%|          | 13/6567 [00:18<3:17:54,  1.81s/it]Training:   0%|          | 14/6567 [00:19<2:50:58,  1.57s/it]Training:   0%|          | 15/6567 [00:21<3:13:41,  1.77s/it]Training:   0%|          | 16/6567 [00:23<3:14:02,  1.78s/it]Training:   0%|          | 17/6567 [00:24<3:02:19,  1.67s/it]Training:   0%|          | 18/6567 [00:25<2:46:40,  1.53s/it]Training:   0%|          | 19/6567 [00:27<2:35:08,  1.42s/it]Training:   0%|          | 20/6567 [00:30<3:39:37,  2.01s/it]Training:   0%|          | 21/6567 [00:32<3:29:37,  1.92s/it]Training:   0%|          | 22/6567 [00:33<3:05:54,  1.70s/it]Training:   0%|          | 23/6567 [00:35<3:11:50,  1.76s/it]Training:   0%|          | 24/6567 [00:36<2:49:27,  1.55s/it]Training:   0%|          | 25/6567 [00:37<2:37:03,  1.44s/it]Training:   0%|          | 26/6567 [00:38<2:25:07,  1.33s/it]Training:   0%|          | 27/6567 [00:39<2:16:04,  1.25s/it]Training:   0%|          | 28/6567 [00:41<2:39:15,  1.46s/it]Training:   0%|          | 29/6567 [00:43<2:38:43,  1.46s/it]Training:   0%|          | 30/6567 [00:44<2:37:47,  1.45s/it]Training:   0%|          | 31/6567 [00:45<2:29:38,  1.37s/it]Training:   0%|          | 32/6567 [00:48<3:34:22,  1.97s/it]Training:   1%|          | 33/6567 [00:50<3:17:15,  1.81s/it]Training:   1%|          | 34/6567 [00:52<3:14:53,  1.79s/it]Training:   1%|          | 35/6567 [00:53<2:55:50,  1.62s/it]Training:   1%|          | 36/6567 [00:54<2:35:06,  1.43s/it]Training:   1%|          | 37/6567 [00:55<2:22:11,  1.31s/it]Training:   1%|          | 38/6567 [00:56<2:19:06,  1.28s/it]Training:   1%|          | 39/6567 [00:57<2:18:00,  1.27s/it]Training:   1%|          | 40/6567 [00:58<2:09:02,  1.19s/it]Training:   1%|          | 41/6567 [01:00<2:08:23,  1.18s/it]Training:   1%|          | 42/6567 [01:01<2:08:14,  1.18s/it]Training:   1%|          | 43/6567 [01:02<2:04:48,  1.15s/it]Training:   1%|          | 44/6567 [01:03<2:08:58,  1.19s/it]Training:   1%|          | 45/6567 [01:05<2:18:21,  1.27s/it]Training:   1%|          | 46/6567 [01:05<2:08:33,  1.18s/it]Training:   1%|          | 47/6567 [01:06<2:02:09,  1.12s/it]Training:   1%|          | 48/6567 [01:08<2:18:50,  1.28s/it]Training:   1%|          | 49/6567 [01:10<2:33:52,  1.42s/it]Training:   1%|          | 50/6567 [01:11<2:21:45,  1.31s/it]Training:   1%|          | 51/6567 [01:12<2:17:30,  1.27s/it]Training:   1%|          | 52/6567 [01:13<2:11:14,  1.21s/it]Training:   1%|          | 53/6567 [01:14<2:05:57,  1.16s/it]Training:   1%|          | 54/6567 [01:16<2:14:29,  1.24s/it]Training:   1%|          | 55/6567 [01:17<2:12:20,  1.22s/it]Training:   1%|          | 56/6567 [01:18<2:04:56,  1.15s/it]Training:   1%|          | 57/6567 [01:19<2:14:48,  1.24s/it]Training:   1%|          | 58/6567 [01:20<2:08:30,  1.18s/it]Training:   1%|          | 59/6567 [01:21<2:03:23,  1.14s/it]Training:   1%|          | 60/6567 [01:22<1:58:08,  1.09s/it]Training:   1%|          | 61/6567 [01:23<2:00:51,  1.11s/it]Training:   1%|          | 62/6567 [01:25<1:59:05,  1.10s/it]Training:   1%|          | 63/6567 [01:26<2:11:20,  1.21s/it]Training:   1%|          | 64/6567 [01:27<2:15:10,  1.25s/it]Training:   1%|          | 65/6567 [01:28<2:09:18,  1.19s/it]Training:   1%|          | 66/6567 [01:29<2:02:03,  1.13s/it]Training:   1%|          | 67/6567 [01:30<1:56:43,  1.08s/it]Training:   1%|          | 68/6567 [01:32<2:02:01,  1.13s/it]Training:   1%|          | 69/6567 [01:33<1:59:12,  1.10s/it]Training:   1%|          | 70/6567 [01:35<2:43:34,  1.51s/it]Training:   1%|          | 71/6567 [01:36<2:29:12,  1.38s/it]Training:   1%|          | 72/6567 [01:37<2:26:22,  1.35s/it]Training:   1%|          | 73/6567 [01:39<2:20:46,  1.30s/it]Training:   1%|          | 74/6567 [01:40<2:10:50,  1.21s/it]Training:   1%|          | 75/6567 [01:41<2:19:16,  1.29s/it]Training:   1%|          | 76/6567 [01:42<2:19:49,  1.29s/it]Training:   1%|          | 77/6567 [01:44<2:19:09,  1.29s/it]Training:   1%|          | 78/6567 [01:45<2:20:12,  1.30s/it]Training:   1%|          | 79/6567 [01:46<2:23:30,  1.33s/it]Training:   1%|          | 80/6567 [01:48<2:26:43,  1.36s/it]Training:   1%|          | 81/6567 [01:49<2:24:08,  1.33s/it]Training:   1%|          | 82/6567 [01:51<2:50:41,  1.58s/it]Training:   1%|▏         | 83/6567 [01:52<2:38:29,  1.47s/it]Training:   1%|▏         | 84/6567 [01:54<2:32:10,  1.41s/it]Training:   1%|▏         | 85/6567 [01:55<2:33:57,  1.43s/it]Training:   1%|▏         | 86/6567 [01:57<2:35:22,  1.44s/it]Training:   1%|▏         | 87/6567 [01:58<2:27:34,  1.37s/it]Training:   1%|▏         | 88/6567 [01:59<2:18:02,  1.28s/it]Training:   1%|▏         | 89/6567 [02:01<2:47:48,  1.55s/it]Training:   1%|▏         | 90/6567 [02:02<2:29:33,  1.39s/it]Training:   1%|▏         | 91/6567 [02:03<2:23:16,  1.33s/it]Training:   1%|▏         | 92/6567 [02:05<2:19:14,  1.29s/it]Training:   1%|▏         | 93/6567 [02:05<2:08:50,  1.19s/it]Training:   1%|▏         | 94/6567 [02:07<2:27:20,  1.37s/it]Training:   1%|▏         | 95/6567 [02:09<2:44:25,  1.52s/it]Training:   1%|▏         | 96/6567 [02:10<2:37:00,  1.46s/it]Training:   1%|▏         | 97/6567 [02:12<2:38:08,  1.47s/it]Training:   1%|▏         | 98/6567 [02:13<2:24:34,  1.34s/it]Training:   2%|▏         | 99/6567 [02:14<2:21:25,  1.31s/it]Training:   2%|▏         | 100/6567 [02:16<2:26:57,  1.36s/it]Training:   2%|▏         | 101/6567 [02:17<2:23:37,  1.33s/it]Training:   2%|▏         | 102/6567 [02:18<2:14:04,  1.24s/it]Training:   2%|▏         | 103/6567 [02:19<2:18:58,  1.29s/it]Training:   2%|▏         | 104/6567 [02:21<2:23:34,  1.33s/it]Training:   2%|▏         | 105/6567 [02:22<2:15:17,  1.26s/it]Training:   2%|▏         | 106/6567 [02:23<2:21:55,  1.32s/it]Training:   2%|▏         | 107/6567 [02:25<2:17:19,  1.28s/it]Training:   2%|▏         | 108/6567 [02:26<2:15:20,  1.26s/it]Training:   2%|▏         | 109/6567 [02:27<2:15:28,  1.26s/it]Training:   2%|▏         | 110/6567 [02:29<2:28:38,  1.38s/it]Training:   2%|▏         | 111/6567 [02:30<2:29:20,  1.39s/it]Training:   2%|▏         | 112/6567 [02:31<2:23:29,  1.33s/it]Training:   2%|▏         | 113/6567 [02:32<2:09:06,  1.20s/it]Training:   2%|▏         | 114/6567 [02:34<2:17:27,  1.28s/it]Training:   2%|▏         | 115/6567 [02:35<2:08:24,  1.19s/it]Training:   2%|▏         | 116/6567 [02:36<2:08:31,  1.20s/it]Training:   2%|▏         | 117/6567 [02:37<2:16:08,  1.27s/it]Training:   2%|▏         | 118/6567 [02:38<2:13:40,  1.24s/it]Training:   2%|▏         | 119/6567 [02:40<2:14:35,  1.25s/it]Training:   2%|▏         | 120/6567 [02:41<2:12:41,  1.23s/it]Training:   2%|▏         | 121/6567 [02:42<2:13:30,  1.24s/it]Training:   2%|▏         | 122/6567 [02:43<2:12:34,  1.23s/it]Training:   2%|▏         | 123/6567 [02:44<2:06:20,  1.18s/it]Training:   2%|▏         | 124/6567 [02:46<2:02:27,  1.14s/it]Training:   2%|▏         | 125/6567 [02:47<2:12:46,  1.24s/it]Training:   2%|▏         | 126/6567 [02:48<2:15:05,  1.26s/it]Training:   2%|▏         | 127/6567 [02:49<2:13:17,  1.24s/it]Training:   2%|▏         | 128/6567 [02:51<2:15:08,  1.26s/it]Training:   2%|▏         | 129/6567 [02:52<2:22:10,  1.32s/it]Training:   2%|▏         | 130/6567 [02:54<2:24:20,  1.35s/it]Training:   2%|▏         | 131/6567 [02:55<2:15:53,  1.27s/it]Training:   2%|▏         | 132/6567 [02:56<2:16:47,  1.28s/it]Training:   2%|▏         | 133/6567 [02:57<2:15:05,  1.26s/it]Training:   2%|▏         | 134/6567 [02:59<2:31:12,  1.41s/it]Training:   2%|▏         | 135/6567 [03:00<2:19:43,  1.30s/it]Training:   2%|▏         | 136/6567 [03:01<2:23:32,  1.34s/it]Training:   2%|▏         | 137/6567 [03:03<2:25:44,  1.36s/it]Training:   2%|▏         | 138/6567 [03:04<2:16:23,  1.27s/it]Training:   2%|▏         | 139/6567 [03:05<2:21:39,  1.32s/it]Training:   2%|▏         | 140/6567 [03:08<2:50:02,  1.59s/it]Training:   2%|▏         | 141/6567 [03:09<2:46:50,  1.56s/it]Training:   2%|▏         | 142/6567 [03:11<2:42:57,  1.52s/it]Training:   2%|▏         | 143/6567 [03:12<2:26:27,  1.37s/it]Training:   2%|▏         | 144/6567 [03:13<2:20:18,  1.31s/it]slurmstepd-gpu008: error: *** JOB 50653447 ON gpu008 CANCELLED AT 2025-12-20T09:31:38 ***
