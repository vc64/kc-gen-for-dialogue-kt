Removing conda version miniforge3-24.7.1
No conda package cache directories found outside your home directory. To
prevent conda from filling up your home directory, you can create a new
directory at `/work/pi_<your_pi_name>/$USER/.conda/pkgs` and reload the module. 
No conda environment directories found outside your home directory. To prevent
conda from filling up your home directory, you can create a new directory at
`/work/pi_<your_pi_name>/$USER/.conda/envs` and reload the module. 
Loading conda version miniforge3-24.7.1
Loading cuda version 12.1
Lmod has detected the following error:  You have attempted to load two
different versions of the same module. You must unload
 the old version before you can load the new one. You can use the command:
 $ module swap cuda/12.1 cuda/12.6
 You can allow automatic swapping (at your own risk) with the environment
variable
 $LMOD_SAME_NAME_AUTO_SWAP ("yes" or "no").

While processing the following module(s):
    Module fullname             Module Filename
    ---------------             ---------------
    cudnn/8.9.7.29-12-cuda12.6  /modules/modulefiles/spack/latest/linux-ubuntu24.04-x86_64/Core/cudnn/8.9.7.29-12-cuda12.6.lua

`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:51, 17.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:36, 18.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 12.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 14.92s/it]
Traceback (most recent call last):
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/main.py", line 71, in <module>
    main()
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/main.py", line 68, in main
    args.func(args)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 127, in train
    return crossval(args, fn)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 92, in crossval
    metrics = fn(args, fold)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/training.py", line 227, in train_lmkt
    model, tokenizer = get_model(args.base_model, False, pt_model_name=args.pt_model_name, r=args.r, lora_alpha=args.lora_alpha, quantize=args.quantize)
  File "/home/vqchen_umass_edu/dialogue-kt/dialogue_kt/models/lm.py", line 54, in get_model
    model = get_peft_model(model, peft_config)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/mapping.py", line 183, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/peft_model.py", line 1542, in __init__
    super().__init__(model, peft_config, adapter_name, **kwargs)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/peft_model.py", line 155, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/tuners/lora/model.py", line 139, in __init__
    super().__init__(model, config, adapter_name)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 175, in __init__
    self.inject_adapter(self.model, adapter_name)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 431, in inject_adapter
    self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/tuners/lora/model.py", line 224, in _create_and_replace
    new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/tuners/lora/model.py", line 340, in _create_new_module
    new_module = dispatcher(target, adapter_name, lora_config=lora_config, **kwargs)
  File "/home/vqchen_umass_edu/.conda/envs/LLMKT/lib/python3.10/site-packages/peft/tuners/lora/bnb.py", line 273, in dispatch_bnb_8bit
    "memory_efficient_backward": target.state.memory_efficient_backward,
AttributeError: 'MatmulLtState' object has no attribute 'memory_efficient_backward'
